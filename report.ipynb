{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "Gas Consumption in France\n",
    "2023-2024\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import yaml\n",
    "import glob\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import year,month,weekofyear\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder, Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read some configurations from the yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    configuration = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'delimiter': '|'},\n",
       " {'schema': 'id_pdv int,cp int,pop string,latitude double,longitude  double,services string'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration['service_file_parameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  = pyspark.sql.SparkSession.builder.appName(\"Gas\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Gas</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fae7464d9d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Read the data from the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the schema for the different datasets using the yaml configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the right delimiter for each each data from the configuration file\n",
    "gas_delimiter = configuration['gas_file_parameters'][0]['delimiter']\n",
    "station_delimiter = configuration['station_file_parameters'][0]['delimiter']\n",
    "service_delimiter = configuration['service_file_parameters'][0]['delimiter']\n",
    "\n",
    "#get the right schema for each data from the configuration file\n",
    "gas_schema = configuration['gas_file_parameters'][1]['schema']\n",
    "station_schema = configuration['station_file_parameters'][1]['schema']\n",
    "service_schema = configuration['service_file_parameters'][1]['schema']\n",
    "\n",
    "#collect the file data paths for each data from the configuration file\n",
    "gas_files = glob.glob(\"data/Prix*.csv\")\n",
    "station_files = glob.glob(\"data/Station*.csv\")\n",
    "service_files = glob.glob(\"data/Service*.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_ddf = spark.read.csv(gas_files, schema=gas_schema, sep=gas_delimiter)\n",
    "station_ddf = spark.read.csv(station_files, schema=station_schema, sep=station_delimiter)\n",
    "service_ddf = spark.read.csv(service_files, schema=service_schema, sep=service_delimiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Preprocessing the Gas data </br>\n",
    "We will be doing the following :</br>\n",
    "* a) Sort date by date column\n",
    "* b) Split the date in year, month and weak of the year\n",
    "* c) Prepare latitude & longitude for mapping (divide by the right power of 10)\n",
    "* d) Create a Table associated with gas data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier we will use a preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.a) Sort the date by date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_pdv: int, cp: int, pop: string, latitude: double, longitude: double, date: timestamp, id_carburant: int, nom_carburant: string, prix: double]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM gas ORDER BY date ASC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.b) Split the date in year, month and weak of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+---------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "| id_pdv|  cp|pop| latitude|longitude|               date|id_carburant|nom_carburant|  prix|year|month|weekofyear|\n",
      "+-------+----+---+---------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "|1000001|1000|  R|4620114.0| 519791.0|2017-01-02 09:37:03|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|4620114.0| 519791.0|2017-01-03 09:54:58|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|4620114.0| 519791.0|2017-01-06 12:33:57|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|4620114.0| 519791.0|2017-01-09 08:59:53|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "|1000001|1000|  R|4620114.0| 519791.0|2017-01-10 10:38:39|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "+-------+----+---+---------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gas_ddf = gas_ddf.withColumn(\"year\",year(gas_ddf.date))\n",
    "gas_ddf = gas_ddf.withColumn(\"month\",month(gas_ddf.date))\n",
    "gas_ddf = gas_ddf.withColumn(\"weekofyear\",weekofyear(gas_ddf.date))\n",
    "gas_ddf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.c) Prepare latitude & longitude for mapping (divide by the right power of 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "| id_pdv|  cp|pop|latitude|longitude|               date|id_carburant|nom_carburant|  prix|year|month|weekofyear|\n",
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-02 09:37:03|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-03 09:54:58|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-06 12:33:57|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-09 08:59:53|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-10 10:38:39|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gas_ddf = gas_ddf.withColumn(\"latitude\",F.col(\"latitude\")/100_000)\n",
    "gas_ddf = gas_ddf.withColumn(\"longitude\",F.col(\"longitude\")/100_000)\n",
    "gas_ddf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.d) Create a Table associated with gas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "| id_pdv|  cp|pop|latitude|longitude|               date|id_carburant|nom_carburant|  prix|year|month|weekofyear|\n",
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-02 09:37:03|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-03 09:54:58|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-06 12:33:57|           1|       Gazole|1258.0|2017|    1|         1|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-09 08:59:53|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "|1000001|1000|  R|46.20114|  5.19791|2017-01-10 10:38:39|           1|       Gazole|1258.0|2017|    1|         2|\n",
      "+-------+----+---+--------+---------+-------------------+------------+-------------+------+----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gas_ddf.createOrReplaceTempView('gas')\n",
    "spark.sql(\"SELECT * FROM gas\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gas types have some interest for the rest of the project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>  (26 + 1) / 27]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------------------+\n",
      "|nom_carburant|   count|               ratio|\n",
      "+-------------+--------+--------------------+\n",
      "|       Gazole|16637600|  0.3404250025361638|\n",
      "|          E10|10517151| 0.21519336658221244|\n",
      "|         SP98|10199139|  0.2086864643903981|\n",
      "|         SP95| 7125074| 0.14578745338993335|\n",
      "|         GPLc| 2182050|0.044647327546282894|\n",
      "|          E85| 2173148|0.044465182082238985|\n",
      "|         NULL|   38864|7.952034727704398E-4|\n",
      "+-------------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT nom_carburant,\n",
    "    count(*) as count,\n",
    "    round(100 * count(*) / (SELECT count(*) FROM gas),2) as ratio\n",
    "    FROM gas \n",
    "    GROUP BY  nom_carburant\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    ).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
